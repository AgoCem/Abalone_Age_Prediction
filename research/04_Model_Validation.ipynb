{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\agost\\\\Data_Career\\\\End_to_end_projects\\\\Abalone_Age_Prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    local_data_file: Path\n",
    "    trained_model_path: Path\n",
    "    best_model_path: Path\n",
    "    param_target_col: str\n",
    "    elastic_pickle: Path\n",
    "    lasso_pickle: Path\n",
    "    lr_pickle: Path\n",
    "    rfr_pickle: Path\n",
    "    ridge_pickle: Path\n",
    "    svr_pickle: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Abalone_Age_Prediction.utils.common import create_directories, read_yaml, load_object, save_object\n",
    "from Abalone_Age_Prediction.constants import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root]) #the artifacts_root is the key of the dictionary created\n",
    "                                                # in the yaml file and we can read this key like that instead of\n",
    "                                                # [\"artifacts_root\"] because we used the ConfigBox in the common.py file\n",
    "\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation #model evaluation is the other key value of the dictionary in the config.yaml file\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            local_data_file = config.local_data_file,\n",
    "            trained_model_path = config.trained_model_path,\n",
    "            best_model_path= config.best_model_path,\n",
    "            param_target_col=self.params.TARGET,\n",
    "            elastic_pickle= config.elastic_pickle,\n",
    "            lasso_pickle= config.lasso_pickle,\n",
    "            lr_pickle= config.lr_pickle,\n",
    "            rfr_pickle= config.rfr_pickle,\n",
    "            ridge_pickle= config.ridge_pickle,\n",
    "            svr_pickle= config.svr_pickle\n",
    "        )                                     \n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "from Abalone_Age_Prediction import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    \n",
    "    def read_file(self):\n",
    "        \"\"\"\n",
    "        Read the csv file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            \n",
    "            testing_data = pd.read_csv(self.config.local_data_file)\n",
    "\n",
    "            return testing_data\n",
    "\n",
    "        except Exception as e:\n",
    "            e\n",
    "\n",
    "    \n",
    "    def evaluation_metrics(self,X_test,y_test, models: dict):\n",
    "        \"\"\"\n",
    "        Evaluate all the models and returns 3 dictionaries with the different evaluation scores\n",
    "        \"\"\"\n",
    "        r2_dic = {}\n",
    "        mse_dic = {}\n",
    "        mae_dic = {}\n",
    "\n",
    "        for key in models.keys():\n",
    "            r2_val =  r2_score(y_test,models[key].predict(X_test))\n",
    "            mse_val = mean_squared_error(y_test,models[key].predict(X_test))\n",
    "            mae_val = mean_absolute_error(y_test,models[key].predict(X_test))\n",
    "            r2_dic[key] = r2_val\n",
    "            mse_dic[key] = mse_val\n",
    "            mae_dic[key] = mae_val\n",
    "        return r2_dic, mse_dic, mae_dic\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def model_evaluation(self,testing_data):\n",
    "        \"\"\"\n",
    "        Loading the data and the models and evaluate them\n",
    "        \"\"\"\n",
    "        X_test = testing_data.drop(labels = self.config.param_target_col, axis = 1)\n",
    "        y_test = testing_data[self.config.param_target_col]\n",
    "\n",
    "        models = {\n",
    "            \"elastic\" : load_object(Path(self.config.elastic_pickle)),\n",
    "            \"lasso\" : load_object(Path(self.config.lasso_pickle)),\n",
    "            \"lr\" : load_object(Path(self.config.lr_pickle)),\n",
    "            \"rfr\" : load_object(Path(self.config.rfr_pickle)),\n",
    "            \"ridge\" : load_object(Path(self.config.ridge_pickle)),\n",
    "            \"svr\" : load_object(Path(self.config.svr_pickle))\n",
    "        }\n",
    "\n",
    "        r2_dic, mse_dic, mae_dic = self.evaluation_metrics(X_test,y_test,models)\n",
    "\n",
    "        best_model = [i for i in r2_dic if r2_dic[i]==max(r2_dic.values())]\n",
    "        best_model_score = max(r2_dic.values())\n",
    "        \n",
    "        logger.info(f\"R2 Score dictionary : {r2_dic} \\n\")\n",
    "        logger.info(f\"MSE dictionary : {mse_dic} \\n\")\n",
    "        logger.info(f\"MAE Score dictionary : {mae_dic} \\n\")\n",
    "        logger.info(f\"The best model is {best_model[0]} with {round(best_model_score,3)} R2 score\")\n",
    "\n",
    "        save_object(Path(self.config.best_model_path), models[best_model[0]],\"best_model.pkl\")\n",
    "        \n",
    "        return None\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-15 17:06:52,002: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-06-15 17:06:52,003: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-06-15 17:06:52,004: INFO: common: The directory artifacts already exists]\n",
      "[2024-06-15 17:06:52,005: INFO: common: The directory artifacts/training already exists]\n",
      "[2024-06-15 17:06:52,441: INFO: 1508143048: R2 Score dictionary : {'elastic': -0.0006478694463609713, 'lasso': -0.0006478694463609713, 'lr': 0.5387176640137978, 'rfr': 0.5589292807597441, 'ridge': 0.5273528291133511, 'svr': 0.4908606722019585} \n",
      "]\n",
      "[2024-06-15 17:06:52,442: INFO: 1508143048: MSE dictionary : {'elastic': 9.90927254962901, 'lasso': 9.90927254962901, 'lr': 4.568012913619503, 'rfr': 4.367860167464115, 'ridge': 4.680557246095193, 'svr': 5.041933849993716} \n",
      "]\n",
      "[2024-06-15 17:06:52,442: INFO: 1508143048: MAE Score dictionary : {'elastic': 2.3395858186458227, 'lasso': 2.3395858186458227, 'lr': 1.5813770314261746, 'rfr': 1.487188995215311, 'ridge': 1.596195994339306, 'svr': 1.5083836468800422} \n",
      "]\n",
      "[2024-06-15 17:06:52,443: INFO: 1508143048: The best model is rfr with 0.559 R2 score]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "    testing_data = model_evaluation.read_file()\n",
    "    model_evaluation.model_evaluation(testing_data)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'key1' : 1,\n",
    "       'key2': 3,\n",
    "       'key3': 2}\n",
    "value = [i for i in dic if dic[i]==max(dic.values())]\n",
    "\n",
    "\n",
    "#max(dic[dic.keys() == max(dic.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
